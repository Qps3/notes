---
title: 第一课
updated: 2022-01-28T22:41:20
created: 2022-01-28T20:27:42
---

excle是一个数据分析工具

![image1](assets/74bba4927885423d91ae3ec001c71072.png)

数据挖掘 ——\>机器学习

![image2](assets/c3f14ae70d274551abef97c2a0781bd0.png)

统一思想：用样本估计总体
总体无法得到，所以利用样本
![image3](assets/e158d63db1c641fcb63ecd6e9b0cc151.png)

随着机器学习的深入，样本越多越好
但是样本太多也不太好

![image4](assets/36acc67ff3004566b1a46dddb1b005ca.png)

![image5](assets/907516a6b3484960af7b2f02dc8154ed.png)

![image6](assets/6bf8f89e29954c149946da7c76b37097.png)

对未知的数据进行运算

凸优化课程
![image7](assets/7095346cc7da4cc5a5d89593a5642651.png)

![image8](assets/ffb987eaacef497eb7ace31ef6b65528.png)
凸集
凸函数
非凸转化为凸问题(数学定理)，找到最优解
加入非tu结果是p\*
tu问题结果是d\*
一般d\*\<P\*
但是我们希望d\*=p\*
此时需要满足一些充分必要条件

此时就可以用tu问题替代掉原来的非tu问题

此时要利用到三个人提出的5个条件
KKT条件——\>很重要
一般会在 xgboost与SVM的非线性运算

求函数的最小值
一阶导数，求驻点。
机器学习的原理是一个优化问题

损失可以理解为误差
![image9](assets/e06df08e1b9247dbb2427439d5dede57.png)
首先做一个得分函数
x多了就是组成超平面
由三位平面直角坐标系，推到到多维超平面
X1~Xn都有了，求a1~an
利用梯度下降

首先瞎猜
都给零，都给1，都给正太分步

此时算出来的y与实际Y有误差

将误差都加起来就是总误差
然后有正的，有负的，那就用平方

![image10](assets/69400e3df32c43b3971f6b1138befd33.png)
让第三个最小，就是让总误差最小
此时的a1~an就是有效的

线性回归就是在不断的优化损失函数，让误差变小
为未来做预测

回归问题是预测具体的数值

因为得分方程是线性方程
所以叫做线性回归

有监督：
原来的数据集里有Y，即有参考
无监督
原来的数据集里没有Y，即无参考
![image11](assets/548803c25dfd408a8a86a163127f9ced.png)

![image12](assets/f58b632d57864e6fb757ac18cc8f49a2.png)

![image13](assets/8fe6df43c4ac4bfbb729518b24e315bf.png)

![image14](assets/63fb3a483c9b4fa082780dd6248b5020.png)
带着2的原因是求导数的时候指数会下去
![image15](assets/11f7837af0d143869ecaec2a825cd684.png)
MSE均方误差

多元线性回归
矩阵化表示

逻辑回归极大似然估计

梯度下降
如何找到a1~an
梯度是一个有方向的量，方向导数
直角坐标系中，向量可以表示一个点
点就是向量

![image16](assets/1e82598abe964cf1bb885fe88bb72230.png)
上述就是梯度
梯度就是其上升最快的方向
其反方向就是函数值下降的最快的方向
沿着负方向到达x轴就是最优值

![image17](assets/6e0c235ac5b14e6f8831ee3567065f01.png)

![image18](assets/9ac5fbaaa25849a6a85a15a034035998.png)![image19](assets/a76de7dd573640828d68c0da584785ea.png)

![image20](assets/9f827f08a4e84a87aa224bc2b4083baf.png)

由等高线
向中间走，喇嘛塔的值每次小一点往中间走，直到误差最小。

每一个得分函数都得到一个误差，等误差收敛
![image21](assets/eb2abe14ab9b48f68bbdfb1fae5d937a.png)
线性回归一定能找到最优值
动量自适应

需要手动调整的都是超参数
例如哪个喇嘛塔

高等数学——\>数学分析
线性代数——\>高等代数
矩阵论✔
学线性代数理解空间与空间的变换
例如坐标变换，利于运算
![image22](assets/d588305032e44bdbbf69afd942d92515.png)

![image23](assets/b4a2d7e413594bbcab140dcab6dd67ef.png)

![image24](assets/c8b1a634e8c14ed982d8f6532457a646.png)
用类似于中间的平面来拟合实际变化

![image25](assets/6853c8db73714c06986c95d5976c1e80.png)
但是过度拟合不可取
即拟合了所有的现有的点，但是对未来的变化没有拟合
![image26](assets/862f96f2b71e4a66930c32c88f1c0277.png)
如果拟合的等高线都一样，也不太可取
画一个园或者菱形与等高线相切，缩小最优解的范围

范数
![image27](assets/b353361173024aff9d01552a64069dd6.png)

欧式几何，两个点之间的距离，
![image28](assets/809eed7700634c099fd8d6a35a0a0778.png)

稀疏最优解，就是对最优解加了一个约束
![image29](assets/050abc8b0cd347599cd40495d5033906.png)

逻辑hi贵
回归——\>预测一个数
分类——\>分类
逻辑回归——\>分类
二分类，多分类问题
![image30](assets/dbb8e605afe54b0fa79440f841e81507.png)
逻辑回归与线性回归的共同点
一次线性回归

![image31](assets/d2148bf4dfb044069cae03b2cc8d7aa9.png)

二分类：让Y等于概率
即让y在0~1之间

sigmoid函数\<——\>激活函数
激活是1
未激活是0
拟合后大于0.5是1
小于0.5是0

那么这就变成了一个分类函数

![image32](assets/63a9ff01e911414db8f38b0c33f6f661.png)
依然用MSA均方误差
![image33](assets/4ea7e05c1fb04af6b5c05893d1ed5779.png)

![image34](assets/80b1dc0af1964cb9b012fddcfbacaba7.png)

实现多分类

![image35](assets/611391ef2a954ae0bbc01ca0335a5f2f.png)

三分类
![image36](assets/e45132ebedae41d29bf2edca19f62bcf.png)

交叉熵
比较两个不同的概率分布之间的额损失
用熵来形容误差
MSE
在多分类的时候不太适用
所以引用交叉熵
但是其中哟log所以是使用e

决策树的引入
![image37](assets/932f2386b8d04cc4900e7a972cf12cc6.png)
决策树是一个分类的模型
也可以做回归——\>回归树
分类与线性回归
树模型的业务解释性很强
但是使用线性回归方程业务解释性就不太强

![image38](assets/e99bef3020414808b13408d5995139e1.png)

![image39](assets/3dc6f453e9d2417f8fe64a00dba7eaa2.png)
度：
在一个数据集里，找到一个变量，用这个变量分类，分类结果很正确
分的做好的放在最上面

熵表示的是信息量，与纯度有相通的地方
信息量描述纯度
用熵描述纯度
用别的信息增益分类，熵当损失函数，可以让结果准确

信息增益初一增益个数，得到增益率

随机森林
![image40](assets/b545e7f403b140bc93614246ee89c6e4.png)

![image41](assets/0f6c61b053cc44569ee54ff10ac896d7.png)

![image42](assets/89309107a0e14093adbf9a9de4ac54e6.png)
下面就是深度学习
卷积——\>提取特征

bin是python的执行文件

重要算法

越不确定，，信息量越大
熵大的先分裂
![image43](assets/a373cec6af524737b0e91a7b63485c52.png)

![image44](assets/9c66598c38a0479da2bf854ff5844708.png)

![image45](assets/c4abd2dced724ac98e0a62144eb33bdd.png)![image46](assets/f84f454eabb64e3c832fa80f6b3fc64c.png)
Anaconda ——\> 可以管理虚拟环境，提供很多IDE
![image47](assets/46c7babec8a4415d818b86db9bf8e7e9.png)

![image48](assets/d482604f4cbe448799e45b9c7c55220b.png)

![image49](assets/7cb927757db549639e0f9151c4ec0b9a.png)
安装python 的库用pip

