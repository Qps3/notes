# 向量数据库技术鉴赏（上）



本节内容，整理于Ｂ站知名ＵＰ主的视频，侵删。

**原作者：Ｂ站ＵＰ主　Ele实验室**

[原视频链接，点击跳转](https://www.bilibili.com/video/BV11a4y1c7SW/?share_source=copy_web&vd_source=b00def97c7de87603d59008e1fe68bd3)





## １.以实例引入基本概念

### 如何区分狗的种类

> 通过观察和分析多个特征来区分不同种类的狗的

1. **狗的区分**：狗的品种可以通过观察各种特征进行区分，如体型大小、毛发长短等。

2. **坐标轴表示**：将每个特征表示为坐标轴，每种狗可以被表示为一个点。

   > <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143138.png" style="zoom:25%;" />

3. **多维度观察**：单一特征可能无法明确区分两种狗，如金毛和拉布拉多的体型相似。因此需要从多个角度或维度来观察狗的特征。

   > <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143188.png" style="zoom:25%;" />

4. **更多特征**：可以引入更多的特征，如鼻子的长度、眼睛的大小等。每增加一个特征，就相当于在高维空间中增加了一个坐标轴。

   > <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143358.png" style="zoom:25%;" />

5. **抽象特征**：除了物理特征外，还可以考虑一些抽象的特征，如狗的服从性和攻击性。

   > <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143286.png" style="zoom:25%;" />

6. **高维空间表示**：通过在高维空间中为每种狗分配一个点，可以使得每种狗的特征更为独特和明确，从而提高区分能力。虽然我们无法在四维空间中绘图，但是可以轻松地在数值上实现。

### 推广到区分其他事物：

> 将上述这种观察和分析狗的特征的方法推广到其他事物
>
> 并且引入了向量的概念。以下是对这段文字的总结：

1. **事物的高维表示**：几乎所有的事物都可以被以高维空间的方式表示，包括具象的自然事物（山、河、日、月、鸟、兽、鱼、虫）和抽象的情感（喜、怒、哀、乐，悲欢离合）。不同的事物在不同的特征维度上有着不同的表现，也就是说，它们在高维特征空间中对应着不同的坐标点。

2. **更高的特征维度**：在更大范围内，可能需要更高的特征维度才能很好地区分事物，可能是几百、几千甚至上万个维度。

3. **事物的空间分布**：在特征空间中，概念上更为接近的点会在空间中更为聚集，而概念上非常不同的点则距离很远。

4. **向量的引入**：如果以坐标原点为起点，这些坐标点为终点，就形成了我们熟悉的带有方向和大小的向量。

   > 这种向量化的表达方式甚至具有一定的推理能力:
   >
   > > 比如警察的向量减去小偷的向量，得到的结果向量和猫减去老鼠的结果向量相似
   > >
   > > 这意味着猫和老鼠的关系类似于警察和小偷的关系。
   > >
   > > <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143036.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-5" style="zoom:25%;" />

5. **向量化的应用**：向量化的数据能够实现许多功能。

   > 例如，
   >
   > 对图片进行向量化：就可以通过搜索相似的向量实现图像搜索的功能；
   >
   > 对视频进行向量化：就可以通过搜索相似的向量实现相关视频的推荐。
   >
   > <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143079.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-6" style="zoom:25%;" />





### 向量化数据应用场景

> 通过向量化数据的应用，引入了词向量和向量数据库的概念。

1. **商品向量化**：对商品进行向量化，可以通过搜索相似的向量，针对用户当前浏览的商品进行相关推荐。

2. **文本向量化**：对文本进行向量化，可以在一个智能问答系统中，根据用户当前的问题，找到一些已经解决过的相似的问题以供参考。

3. **词向量**：词向量是近年来在自然语言处理领域得到广泛应用的一种向量化表示方式。训练合适的词向量可以发现自然语言中所蕴含的实际概念。

4. **大语言模型**：以 GPT 为代表的大语言模型通过向量化处理语言数据，可以用当前的对话搜索到历史中最为相似的一些对话，也就是找到和当前对话最相关的记忆，这将极大地提高其输出的效果。

5. **向量数据库**：随着 AI 的发展，向量数据变得越来越重要。然而，传统的数据库并不适合用来存储和检索向量数据，因此需要一种专门的向量数据库。向量数据库的倡导者和创业者们正在基于这样的设想开展他们的工作。



## ２.向量数据库的搜索算法

### 简单引入搜索算法

1. **向量数据库**：向量数据库存储的是向量数据，查询过程则是从库中搜索出和查询向量最为相似的一些向量。它具有一定的模糊性，因此可能会成为未来数据层面的基础设施。

2. **最近邻搜索算法**：向量数据的一个主要应用场景是最近邻搜索，即给定一个查询向量，然后从众多的向量中找到最为相似的一些。实现这一点的方法被称为最近邻搜索算法。

   > <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143734.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-7" style="zoom:25%;" />

3. **暴力搜索**：最简单的最近邻搜索算法就是暴力搜索，也叫平坦搜索。它通过依次比较所有向量和查询向量的相似度，挑选出相似度最高的几个。

   > <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143552.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-8" style="zoom:25%;" />
   >
   > 比较两个向量的相似度的具体方法有很多:
   >
   > > 比如计算向量夹角的余弦值，或直接计算两个向量的欧式距离。
   > >
   > > <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143078.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-10" style="zoom:25%;" />
   > >
   > > <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143099.png" style="zoom:25%;" />

4. **暴力搜索的优缺点**：虽然暴力搜索会导致极高的搜索时间，但它的搜索质量是完美的，因为它真的比较了每一个向量。所以，如果库中数据规模较小，并且你可以接受全部向量的搜索时间，那么暴力搜索是一种好的方法。然而，在实际应用中，数据规模往往都不会太小，因此需要找到更有效的搜索方法。





### １.聚类算法

#### 以 K-means 为例

>  聚类算法在优化向量搜索中的应用：解决暴力搜索的痛点

1. **优化思路**：为了优化向量搜索，一种朴素的想法是为查询向量先划定一个大致的范围，这样就可以缩小搜索的范围。通过这种方式，可能将千万级别的查找次数降到只有几十万的级别。
2. **聚类算法**：聚类算法可以实现这种优化思路。以最流行的 K-means 聚类算法为例，首先选定一个想要分类的数量，然后随机生成相应数量的点，称为聚类中心。
3. **K-means 过程**：在 K-means 算法中，每个向量和哪个聚类中心最近，就被分为哪一类。

> 然后，计算当前同一类的向量的平均点，并将对应的聚类中心更新为这个平均点。
>
> 然后再次判断每个向量点和哪个聚类中心最近，并重新分类。
>
> 这个过程不断迭代，称为训练。最后，这些聚类中心会趋于稳定，或者说收敛，最终将所有的向量分为若干类

4. **K-means聚类算法**

   1. 首先选定分类数量（4类）

      - 随机生成4个点（聚类中心点）

        <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143727.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-12" style="zoom:25%;" />

   2. 按照到中心点举例对点分类

      <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143045.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-13" style="zoom:25%;" />

   3. 在每一类点族中，计算一个平均向量点

      - 将对应中心点更新到平均点，在重新对点按距离分类

        <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143013.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-14" style="zoom:25%;" />

        <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143375.png" style="zoom:25%;" />

4. 重复以上操作，直到所有中心点趋于稳定
5. 注意：该方法也可能出现失误

<img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143376.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-16" style="zoom:25%;" />



#### 聚类算法缺陷及改进

> 介绍了另一种优化搜索的方法——基于哈希的方法

1. **搜索遗漏问题**：在聚类算法中，可能出现遗漏的问题。比如查询向量和某个聚类中心最近，但和查询向量真正最近的向量可能在另一个聚类中。

2. **解决遗漏问题**：有一些方法可以缓解这个问题

   > 比如增加聚类的数量，同时指定搜索多个最近的区域以减少遗漏。
   >
   > 但这基本上都会增加搜索的时间。
   >
   > 实际上，搜索速度和质量往往是一对难以调和的矛盾，所有算法都是在这两个指标上结合实际情况衡量的结果。
   >
   > **搜索质量与时间反比**
   >
   > <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143557.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-17" style="zoom:25%;" />

3. **近似最近邻搜索**：除了暴力搜索能够确保找到最近邻的一些向量外，其他任何方法都不能保证这一点，而只能得到一些近似的结果。因此，这些算法一般也被称为近似最近邻算法。

4. **基于哈希的方法**：除了聚类，还有一种基于哈希的方法可以减少搜索范围。

   > 哈希：是一种将任何数据映射到固定长度输出的方法。
   >
   > > <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143645.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-18" style="zoom:25%;" />
   > >
   > > <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143023.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-19" style="zoom:25%;" />
   >
   > 碰撞：由于输入任意的数据，输出是固定长度的数，因此会出现数据不同但哈希值相同的情况，被称为碰撞。（例如：鸽笼问题）

### ２.位置敏感哈希算法

#### 位置敏感哈希函数

> 基于哈希的方法的具体实现——位置敏感的哈希函数

1. **位置敏感的哈希函数**：这种哈希函数设计的目标是增大碰撞的可能，因为**哈希碰撞正是分组的依据**。哈希值一样的向量被分到同一组，这些分组也被称为桶。除了容易发生碰撞外，这个哈希函数还具备这样的特性：位置越近，或者说越相似的向量发生碰撞的概率越高，被分到同一个桶中的可能性越大。

2. **查询过程**：在查询的时候，只需要计算查询向量的哈希值，找到其所在的桶，再在这个桶中搜索就好了。因为和查询向量最相似的向量大概率都在这个桶中。

3. **实现方法**：实现位置敏感的哈希函数的一种常用方法是随机生成一些直线，根据向量所在的直线的正反侧得到一个二进制编码。例如，随机生成四条直线，对于每条直线，如果一个向量在这条线的正侧，那么对应位为1；如果在反侧，那么对应位为0。这样，就为每个向量算出了一个四位的二进制编码。

   <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143808.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-20" style="zoom:25%;" />

   <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143859.png" style="zoom:25%;" />

   <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143487.png" style="zoom:25%;" />

#### 位置敏感哈希原理

> 位置敏感哈希函数的直观理解和为什么向量越接近，得到的二进制编码就越相似

1. **二进制编码的相似程度**：通过位置敏感哈希函数计算得到的二进制编码，距离越近的向量其编码越相似。比如，距离近的两个向量A和C的编码只有一个位不同，相似度高达3/4；而距离较远的向量B和A、C的编码相似度都很低。

2. **直观解释**：在计算二进制编码的过程中，如果想让两个向量的某一位编码相同，那么对应的直线必须从这两个向量和第三个向量之间穿过。直线是随机生成的，所以从概率上来说，生成的直线从距离较远的两个向量之间穿过的可能性要比从距离较近的两个向量之间穿过的可能性更大。因此，距离近的两个向量的二进制编码更有可能相同。

3. **哈希值**：这串二进制编码便可以作为向量的哈希值，而生成这串二进制编码的过程就是一种位置敏感的哈希函数。



#### 位置敏感哈希生成原理

> 通过哈希函数为多个向量生成二进制编码，以及在更高维度中的哈希值生成

1. **生成哈希值**：每个向量通过哈希函数后都会得到一串二进制编码的哈希值。那些非常接近的向量，它们的哈希值大概率是一样的，也就被分到了同一个桶中。

2. **更高维度的向量**：对于更高维度的向量，道理也是一样的。比如在三维中，可以使用三维空间中的一个随机平面来做哈希函数的计算。这个平面也有正反两面，正面的向量得到一，反面的向量得到零。若干个随机平面就可以得到一串二进制编码的哈希值。

   <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143676.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-23" style="zoom:25%;" />

   <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143573.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-24" style="zoom:25%;" />

   

3. **更高维度的理解**：如果是更高的维度，虽然我们无法做出图形，但可以理解，在这些更高维度中也存在着包围的超平面，同样也可以完成这样的哈希值生成。所以我们把这种方法称之为随机超平面或者随机投影。

   <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143831.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-25" style="zoom:25%;" />

4. **搜索质量的降低**：除了直接暴力搜索，任何试图减少搜索量的方法都会在一定程度上降低搜索的质量。例如，如果我们用九条随机直线为每个向量生成长度为九的二进制哈希编码，那么可能会有一些接近但不完全相同的向量被分到不同的桶中。



#### 哈希的问题与改进

> 通过分段措施来改善哈希函数可能带来的问题。以下是对这段文字的总结：

1. **哈希碰撞的随机性**：由于直线是随机的，我们无法保证总是能让接近的向量被分到同一个桶中。例如，在生成二进制编码的过程中，有可能随机到一个直线恰好从两个接近的向量之间穿过，导致它们的哈希编码在这一位上不同，最后无法被分到同一个桶中。

   <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143456.png" style="zoom:25%;" />

2. **分段措施**：为了改善这种情况，一般会采用分段的措施。我们可以把二进制哈希值分成几段，然后独立地对这些片段进行分桶。虽然有些片段可能会被分到不同的桶中，但只要有一个片段被分到同一个桶中，我们就可以将这两个向量视为候选的相似向量。

   <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143364.png" style="zoom:25%;" />

   <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143200.png" style="zoom:25%;" />

   <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143206.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-29" style="zoom:25%;" />

   <img src="https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120143382.png" alt="2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-30" style="zoom:25%;" />

3. **搜索范围的合理扩充**：这种分段的措施实际上是对搜索范围的一种合理扩充。如果两个向量比较接近，那么它们就有可能在某一段的哈希值上相同，从而被分到同一个桶中。而那些距离较远的向量，则不太可能有任何一段哈希值相同，因此不会被错误地视为相似向量。

   



## 总体内容总结：

1. **狗的特征与向量**：文章以如何通过观察和分析多个特征来区分不同种类的狗为例，引入了向量的概念。每个特征可以被视为一个维度，每种狗可以被表示为一个点在高维空间中。

2. **事物的高维表示**：这种方法可以被推广到其他事物，包括具象的自然事物和抽象的情感。在特征空间中，概念上更为接近的点会在空间中更为聚集，形成向量。

3. **向量化的应用**：向量化的数据能够实现许多功能。例如，通过向量化处理语言数据，大语言模型如GPT可以通过向量搜索历史对话，提高其输出的效果。

4. **向量数据库与最近邻搜索**：向量数据库存储的是向量数据，主要的查询过程是最近邻搜索。最简单的最近邻搜索方法是暴力搜索，但其效率低下。为了优化搜索，需要采用聚类或基于哈希的方法。

5. **基于哈希的方法**：基于哈希的方法中，设计了一种位置敏感的哈希函数，根据向量位置生成二进制编码作为哈希值。在查询时，只需要计算查询向量的哈希值，然后在相应的桶中进行搜索。

6. **分段哈希**：由于哈希函数可能导致一些接近的向量被分到不同的桶中，为改善这种情况，可以采用分段的措施。将二进制哈希值分成几段，独立地对这些片段进行分桶。只要有一个片段被分到同一个桶中，我们就可以将这两个向量视为候选的相似向量。

**参考文献及注意事项：**

> ![2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-31](https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120216717.png)
>
> ![2720a4c3-ca1e-4bd9-bbd0-ee991ce70dfe-32](https://cnchu-1310638968.cos.ap-nanjing.myqcloud.com/2023%20/pic/202307120216006.png)
>
> [原视频链接，点击跳转](https://www.bilibili.com/video/BV11a4y1c7SW/?share_source=copy_web&vd_source=b00def97c7de87603d59008e1fe68bd3)

The end

